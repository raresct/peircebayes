#!/usr/bin/env python

import numpy as np
from numpy.random import dirichlet
from scipy.sparse import lil_matrix
from scipy.stats import bernoulli
import matplotlib.pyplot as plt
import itertools
import copy

# pb imports
from knowledge_compilation import PyCUDDUtil

# logging stuff
import logging
from logging import debug as logd

# timing stuff
import time

# inference class
class PBInference:
    def __init__(self, pb_model):
        # input
        self.pb_model       = pb_model
        self.myhash         = PyCUDDUtil.cudd_hash
        # important attributes
        self.x              = None
        self.theta          = None
        # other
        self.bdd_param      = None
        self.theta_avg      = None
        self.beta           = None

    def infer():
        self.gibbs_sampler_plates()

    def exact_inference(self):
        self.backward(self.bdd.formula.prob_index)
        return self.beta[(self.bdd.h_nodes[-1], 1)]
    def forward(self, theta):
        # abbrev
        root = self.bdd.root
        nodes = self.bdd.nodes
        h_nodes = self.bdd.h_nodes
        # init alpha and alpha(root)=1
        size = len(nodes)
        self._alpha = dict.fromkeys(
            zip(list(h_nodes)*2, [0]*size+[1]*size),
            0)
        self.alpha = dict.fromkeys(list(h_nodes), 0)
        h_root = self.myhash(root)
        self._alpha[(h_root,0)], self._alpha[(h_root,1)] = 0., 1.
        if root.IsComplement():
            self._alpha[(h_root,0)], self._alpha[(h_root,1)] = 1., 0.
        for curr_node in reversed(nodes):
            h_curr = self.myhash(curr_node)
            logd('curr node: {}'.format(h_curr))
            if not curr_node.IsConstant():
                # get children
                high, low = curr_node.T(), curr_node.E()
                curr_theta = theta[curr_node.NodeReadIndex()]
                h_high, h_low = map(self.myhash, [high, low])
                if low.IsConstant():
                    logd('LEAF NODE')
                # update alpha
                if low.IsComplement():
                    self._alpha[(h_low,1)] += (
                        self._alpha[(h_curr,0)]*(1-curr_theta))
                    self._alpha[(h_low,0)] += (
                        self._alpha[(h_curr,1)]*(1-curr_theta))
                else:
                    self._alpha[(h_low,1)] += (
                        self._alpha[(h_curr,1)]*(1-curr_theta))
                    self._alpha[(h_low,0)] += (
                        self._alpha[(h_curr,0)]*(1-curr_theta))
                self._alpha[(h_high, 1)] += (
                    self._alpha[(h_curr,1)]*curr_theta)
                self._alpha[(h_high, 0)] += (
                    self._alpha[(h_curr,0)]*curr_theta)
                # we are done with h_curr, so we can update its alpha
                self.alpha[h_curr] = (self._alpha[(h_curr,0)] +
                    self._alpha[(h_curr,1)])
            else:
                self.alpha[h_curr] = self._alpha[(h_curr,1)]

    def backward(self, theta):
        # abbrev
        root = self.bdd.root
        nodes = self.bdd.nodes
        h_nodes = self.bdd.h_nodes
        # init beta
        h_one, h_root = [h_nodes[i] for i in [0,-1]]
        self.beta = {(h_one,0):0, (h_one,1):1}
        if root.IsComplement():
            self.beta = {(h_one,0):1, (h_one,1):0}
        for curr_node in nodes[1:]:
            h_curr = self.myhash(curr_node)
            high, low = curr_node.T(), curr_node.E()
            #logd('curr_node_index: {}'.format(curr_node.NodeReadIndex()))
            curr_theta = theta[curr_node.NodeReadIndex()]
            h_high, h_low = map(self.myhash, [high, low])
            if low.IsComplement():
                self.beta[(h_curr,1)] = (curr_theta*self.beta[(h_high,1)]+
                    (1-curr_theta)*self.beta[(h_low, 0)])
                self.beta[(h_curr,0)] = (curr_theta*self.beta[(h_high,0)]+
                    (1-curr_theta)*self.beta[(h_low, 1)])
            else:
                self.beta[(h_curr,1)] = (curr_theta*self.beta[(h_high,1)]+
                    (1-curr_theta)*self.beta[(h_low, 1)])
                self.beta[(h_curr,0)] = (curr_theta*self.beta[(h_high,0)]+
                    (1-curr_theta)*self.beta[(h_low, 0)])

    def backward_plates(self):
        return [self.backward_plate(pb_plate, plate_idx)
            for (plate_idx, pb_plate) in enumerate(self.pb_model.plates)]

    def backward_plate(self, pb_plate, plate_idx):
        bdd = pb_plate.bdd
        plate = pb_plate.plate
        h_one, h_root = [bdd.h_nodes[i] for i in [0,-1]]
        beta_init = {(h_one,0):0, (h_one,1):1}
        if bdd.root.IsComplement():
            beta_init = {(h_one,0):1, (h_one,1):0}
        beta = [beta_init.copy() for i in range(len(plate))]
        for curr_node, h_curr, idx_curr in zip(
            bdd.nodes[1:], bdd.h_nodes[1:], bdd.idx_nodes[1:]):
            high, low = curr_node.T(), curr_node.E()
            h_high, h_low = map(self.myhash, [high, low])
            for plate_row in range(plate.shape[0]):
                curr_theta = self.bdd_param[plate_idx][plate_row, idx_curr]
                if low.IsComplement():
                    beta[plate_row][(h_curr,1)] = (
                        curr_theta*beta[plate_row][(h_high,1)]+
                        (1-curr_theta)*beta[plate_row][(h_low, 0)])
                    beta[plate_row][(h_curr,0)] = (
                        curr_theta*beta[plate_row][(h_high,0)]+
                        (1-curr_theta)*beta[plate_row][(h_low, 1)])
                else:
                    beta[plate_row][(h_curr,1)] = (
                        curr_theta*beta[plate_row][(h_high,1)]+
                        (1-curr_theta)*beta[plate_row][(h_low, 1)])
                    beta[plate_row][(h_curr,0)] = (
                        curr_theta*beta[plate_row][(h_high,0)]+
                        (1-curr_theta)*beta[plate_row][(h_low, 0)])
        return beta

    # TODO check these
    # bdd params, bdd reps
    def sample_bdd_plates(self):
        #start = time.clock()
        self.reset_x()
        betas = self.backward_plates()
        #end = time.clock()
        #logd('beta time: {} s'.format(end-start))
        #start = time.clock()
        for (plate_idx, (pb_plate, beta)) in enumerate(zip(
            self.pb_model.plates, betas)):
            self.sample_bdd_plate(pb_plate, beta, plate_idx)
        #end = time.clock()
        #logd('sample bdd time: {} s'.format(end-start))

    def sample_bdd_plate(self, pb_plate, beta, plate_idx):
        # bdd_reps is a dict of key=hash(node), val = (index, repetitions)
        # bdd_reps is an array of shape (NNodes, NParamSets)
        # bdd_reps[node_number_bft, plate_row] = n_reps
        bdd = pb_plate.bdd
        reps = pb_plate.reps
        NNodes = len(bdd.nodes[1:])
        NParamSets = reps.shape[0]
        self.bdd_reps = np.zeros((NNodes, NParamSets))
        # 0 for root
        self.bdd_reps[0,:] = reps
        nodes = list(reversed(bdd.nodes[1:]))
        h_nodes = list(reversed(bdd.h_nodes[1:]))
        i = 0
        for (node_no_bft, (curr_node,h_curr)) in enumerate(zip(
            nodes,h_nodes)):
            #start = time.clock()
            self.sample_node_2(node_no_bft, curr_node, h_curr,
                pb_plate, beta, plate_idx, h_nodes)
            #end = time.clock()
            #logd('per node time: {} s'.format(end-start))
            #for j in range(self.bdd_reps.shape[0]):
            #    print self.bdd_reps[j,:]
            i+=1
            #if i>4:
            #    return
        #print self.x

    def update_x_high_count(self, samples):
        i,j,k,samples_high = samples
        self.x[i][j,k] += samples_high
        return 0

    def update_x_low_count(self, samples):
        i,j,k,samples_low = samples
        if samples_low>0 and self.theta[i].shape[1]-2 == k:
            self.x[i][j,k+1] += samples_low
        return 0

    def sample_node_3(self, node_no_bft, curr_node, h_curr,
        pb_plate, beta, plate_idx, h_nodes):
        # TODO this doesn't work currently, find out why and fix it
        # no more curr_reps we need bdd_reps[row] instead
        high, low = curr_node.T(), curr_node.E()
        h_high, h_low = map(self.myhash, [high, low])

        # must compute
        # TODO precompute as much as possible
        reps = self.bdd_reps[node_no_bft,:].reshape(-1,1)
        ber_args = np.hstack(
        # this is p1
        ((self.bdd_param[plate_idx][:, node_no_bft]*
            np.array([beta_row[(h_high,1)] for beta_row in beta])
            /np.array([beta_row[(h_curr,1)] for beta_row in beta]))
            .reshape(-1,1),
        # stacked with reps 
        reps))
        samples = np.apply_along_axis(
            lambda x: np.random.multinomial(x[1], [x[0], None]),
            1, ber_args) 

        # TODO figure out how to update self.x with vectorization
        # idea: group by i in (i,j,k) and work with the matrices separately
        #print self.x
        np.apply_along_axis(self.update_x_high_count,1,
            np.hstack((pb_plate.plate[:, node_no_bft], samples[:,[0]]))
            .astype(int))
        np.apply_along_axis(self.update_x_low_count,1,
            np.hstack((pb_plate.plate[:, node_no_bft], samples[:,[1]]))
            .astype(int))
        start_upd_reps = time.clock()
        try:
            node_no_high = h_nodes.index(h_high)
            self.bdd_reps[node_no_high,:] += samples[:,[0]]
        except:
            pass # leaf node
        try:
            node_no_low = h_nodes.index(h_low)
            self.bdd_reps[node_no_low,:] += samples[:,[1]]
        except:
            pass # leaf node
        end_upd_reps = time.clock()
        #logd('per update time: {} s'.format(end_upd_reps-start_upd_reps))


    def sample_node_2(self, node_no_bft, curr_node, h_curr,
        pb_plate, beta, plate_idx, h_nodes):
        # no more curr_reps we need bdd_reps[row] instead
        high, low = curr_node.T(), curr_node.E()
        h_high, h_low = map(self.myhash, [high, low])

        # must compute
        # TODO precompute as much as possible
        curr_theta = self.bdd_param[plate_idx][:, node_no_bft]
        #print curr_theta.shape
        prob_curr_node = np.array([beta_row[(h_curr,1)] for beta_row in beta])
        #print prob_curr_node.shape
        prob_child_1 = np.array([beta_row[(h_high,1)] for beta_row in beta])
        #print prob_child_1.shape
        # P(N sampled 1) = P(N)*beta(Child^1[N])/beta(N)
        p1 = curr_theta*prob_child_1/prob_curr_node
        p1 = p1.reshape(p1.shape[0], 1)
        reps = self.bdd_reps[node_no_bft,:]
        reps = reps.reshape(reps.shape[0],1)
        ber_args = np.hstack((p1, reps))
        #print ber_args.shape
        #print p1[:10]
        #print reps[:10]
        samples = np.apply_along_axis(
            lambda x: np.random.multinomial(x[1], [x[0], None]),
            1, ber_args)
        samples_high = samples[:,[0]]
        samples_low = samples[:,[1]]
        #print samples_low[:10]
        #print
        #print samples_high[:10]
        #print samples_high_col.shape
        #print formula.plate[:, node_no_bft].shape
        #print np.hstack((samples_high_col, formula.plate[:, node_no_bft])).shape

        # TODO figure out how to update self.x with vectorization
        # idea: group by i in (i,j,k) and work with the matrices separately
        #print self.x
        np.apply_along_axis(self.update_x_high_count,1,
            np.hstack((pb_plate.plate[:, node_no_bft], samples_high))
            .astype(int))
        #print self.x[0]
        np.apply_along_axis(self.update_x_low_count,1,
            np.hstack((pb_plate.plate[:, node_no_bft], samples_low))
            .astype(int))

        #node_no_high = h_nodes.index(h_high)
        #reps_high = self.bdd_reps[node_no_high,:]
        #reps_high = reps_high.reshape(reps_high.shape[0],1)
        #reps_high += samples_high
        # update reps
        start_upd_reps = time.clock()
        try:
            node_no_high = h_nodes.index(h_high)
            reps_high = self.bdd_reps[node_no_high,:]
            reps_high = reps_high.reshape(reps_high.shape[0],1)
            reps_high += samples_high
        except:
            pass # leaf node
        try:
            node_no_low = h_nodes.index(h_low)
            reps_low = self.bdd_reps[node_no_low,:]
            reps_low = reps_low.reshape(reps_low.shape[0],1)
            reps_low += samples_low
        except:
            pass # leaf node
        end_upd_reps = time.clock()
        #logd('per update time: {} s'.format(end_upd_reps-start_upd_reps))


    def sample_node_1(self, node_no_bft, curr_node, h_curr,
        formula, beta, plate_idx, h_nodes):
        # no more curr_reps we need bdd_reps[row] instead
        high, low = curr_node.T(), curr_node.E()
        h_high, h_low = map(self.myhash, [high, low])
        creps = self.bdd_reps[node_no_bft].tocoo()
        for _,plate_row, rep in itertools.izip(
            creps.row, creps.col, creps.data):
            i,j,k = formula.plate[plate_row, node_no_bft]
            #if plate_row==2 and idx_curr == 1:
            #    print i,j,k,idx_curr
            #    print self.theta[i].shape[1]
            #    return
            curr_theta = self.bdd_param[plate_idx][plate_row, node_no_bft]
            prob_curr_node = beta[plate_row][(h_curr,1)]
            prob_child_1 = beta[plate_row][(h_high,1)]
            # P(N sampled 1) = P(N)*beta(Child^1[N])/beta(N)
            p1 = curr_theta*prob_child_1/prob_curr_node
            ber_samples = bernoulli.rvs(p1, size=rep)
            samples_high = np.sum(ber_samples)
            self.x[i][j,k] += samples_high
            # deal with the last variable of the pds
            samples_low = rep-samples_high
            # bugged = shape-2
            if samples_low>0 and self.theta[i].shape[1]-2 == k:
                self.x[i][j,k+1] += samples_low
            # update reps
            start_upd_reps = time.clock()
            if samples_high > 0:
                try:
                    node_no_high = h_nodes.index(h_high)
                    self.bdd_reps[node_no_high,plate_row] += samples_high
                except:
                    pass # leaf node
            if samples_low > 0:
                try:
                    node_no_low = h_nodes.index(h_low)
                    self.bdd_reps[node_no_low,plate_row] += samples_low
                except:
                    pass # leaf node
            end_upd_reps = time.clock()
            #logd('per update time: {} s'.format(end_upd_reps-start_upd_reps))


    def reset_x(self):
        distribs = self.pb_model.distribs
        self.x = [np.zeros((distrib[2], distrib[1])) for distrib in distribs]

    def gibbs_sampler_plates(self, n, burn_in, lag, track_ll, in_file):
        start_gibbs = time.time()
        
        n = n+burn_in # n is total number of iterations
        
        thetas = []
        if track_ll:
            lls = []

        # main loop
        for i in range(n):
            logd('Sampling iteration: {}'.format(i))
            start = time.clock()
            # SAMPLE THETA
            #logd('SAMPLE THETA')
            self.sample_theta()
            end = time.clock()
            #logd('sample theta time: {} s'.format(end-start))
            #logd('theta')
            #logd(self.theta)
            thetas.append(self.theta)
            # SAMPLE X
            #logd('SAMPLE X')
            start = time.clock()
            self.sample_bdd_plates()
            end = time.clock()
            #logd('sample x time: {} s'.format(end-start))
            if track_ll:
                lls.append(self.likelihood())
            #logd('x')
            #logd(self.x)
        #self.plot_phi(theta)
        end_gibbs = time.time()
        logd('Sampling time: {} s '.format(end_gibbs-start_gibbs))

        # POST PROCESS
        if burn_in is not None and int(burn_in)>=0:
            thetas = thetas[burn_in:]
        if lag is not None and int(lag)>0:
            thetas = [theta for i,theta in enumerate(thetas) if i%lag==0]
        logd('No. of samples: {}'.format(len(thetas)))

        # avg_theta
        sum_thetas = [np.zeros(theta_d.shape) for theta_d in self.theta]
        for theta_it in thetas:
            for i, theta_d in enumerate(theta_it):
                sum_thetas[i] += theta_d
        self.theta_avg = [theta_d/float(len(thetas)) for theta_d in sum_thetas]
        logd('theta avg')
        for theta_it in self.theta_avg:
            logd(theta_it)

        # ll tracking post-process
        if track_ll:
            self.plot_ll(lls, burn_in, in_file)
            np.savez('/tmp/peircebayes/lls', **{'lls':np.array(lls)})
            #with open('ll_aprob', 'w') as fout:
            #    for ll in lls:
            #        fout.write('{} '.format(ll))

    def plot_ll(self, lls, burn_in, in_file):
        #lls = lls[1:]
        import matplotlib.pyplot as plt
        plt.figure()
        plt.plot(list(range(len(lls))), lls)
        plt.xlabel('Iterations')
        plt.ylabel('Log Likelihood')
        plt.title('PB in_file={}'.format(in_file))
        if burn_in > 0:
            plt.axvline(x=burn_in, linewidth=1, color='red', label='Burn in')
            plt.legend(loc='lower right')
        # TODO plot lag as well 
        plt.savefig('/tmp/peircebayes/ll.pdf', format='pdf')

    def sample_theta(self):
        #start = time.clock()
        distribs = self.pb_model.distribs
        if self.x is None:
            self.reset_x()

        # SAMPLE THETA
        alpha = [np.tile(distrib[0], (distrib[2],1))+
                x_distrib
                for x_distrib, distrib in zip(self.x,distribs)]
        self.theta = [np.array(
            [dirichlet(alpha_row,1)[0] for alpha_row in alpha_d]
            ).reshape(alpha_d.shape)
            for alpha_d in alpha]
        #end = time.clock()
        #logd('alpha to theta: {} s '.format(end-start))
        #print 'theta d2'
        #print self.theta[0][1,:]
        #print 'phi w2'
        #print self.theta[1][:,1]
        #print 'phi w4'
        #print self.theta[1][:,3]

        # REPARAM
        # update bdd_param for all plates
        #start = time.clock()
        self.reparam()
        #end = time.clock()
        #logd('reparam: {} s '.format(end-start))

    def reparam(self):
        self.bdd_param = []
        for pb_plate in self.pb_model.plates:
            bdd_param_plate = np.zeros(pb_plate.plate.shape[:-1])
            for n,cat_d in enumerate(pb_plate.cat_list):
                # build reparams
                P = copy.copy(cat_d)
                for (i,j),L in cat_d.iteritems():
                    if len(L) == 1:
                        P[(i,j)] = [self.theta[i][j,L[0]]]
                    else:
                        P[(i,j)] = self.reparam_row(self.theta[i][j,L])
                # apply them
                for m in range(pb_plate.plate.shape[1]):
                    i,j,k = pb_plate.plate[n,m]
                    bdd_param_plate[n,m] = P[(i,j)][pb_plate.kid[n,m]]
            self.bdd_param.append(bdd_param_plate)

    def reparam_row(self, params):
        params = params.reshape(-1, 1)
        all_params = np.hstack((params, np.ones(params.shape)))
        for j in range(params.shape[0]-1):
            all_params[j,1] = all_params[j,0]/np.prod(1-all_params[:j,1])
        return all_params[:-1,1]

    #def reparam_row(self, bdd_param_plate, row_idx, row, logic_param):
    #    for (col_idx, (i,j,k)) in enumerate(row):
    #        #if row_idx==2:
    #        #    print i,j,k,logic_param[(i,j)]
    #        if self.param_cache.has_key((i,j,k)):
    #            bdd_param_plate[row_idx, col_idx] = self.param_cache[(i,j,k)]
    #        else:
    #            param = self.theta_to_bdd_param(
    #                i,j,k,logic_param[(i,j)])
    #            bdd_param_plate[row_idx, col_idx] = param
    #            self.param_cache[(i,j,k)] = param

    #def theta_to_bdd_param(self, i,j,k, categories):
    #    t_idx = [c for c in categories if c<=k]
    #    reparam = np.ones(len(t_idx)) # for easy debug
    #    for idx in range(len(reparam)):
    #        Z = np.prod(map(lambda x: 1-x, reparam[:idx]))
    #        reparam[idx] = self.theta[i][j,t_idx[idx]]/Z
    #    return reparam[-1]

    def likelihood(self):
        return self.likelihood_lda()
        # general likelihood - modify this to only obs vars
        log_ll = 0
        for x_distrib, theta_distrib in zip(self.x, self.theta):
            for x_val, theta_val in zip(x_distrib.flatten(),
                theta_distrib.flatten()):
                log_ll += x_val*np.log(theta_val)
        return log_ll
    def likelihood_lda(self):
        log_ll = 0
        for x_val, theta_val in zip(self.x[1].flatten(),
            self.theta[1].flatten()):
            log_ll += x_val*np.log(theta_val)
        return log_ll

def main():
    pass
if __name__=='__main__':
    main()

